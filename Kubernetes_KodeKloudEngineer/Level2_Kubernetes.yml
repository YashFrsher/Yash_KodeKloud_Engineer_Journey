Question 1: We are working on an application that will be deployed on multiple containers within a pod on Kubernetes cluster. There is a requirement to share a volume among the containers to save some temporary data. The Nautilus DevOps team is developing a similar template to replicate the scenario. Below you can find more details about it.

1. Create a pod named volume-share-devops.
2. For the first container, use image debian with latest tag only and remember to mention the tag i.e debian:latest, container should be named as volume-container-devops-1, and run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/media.
3. For the second container, use image debian with the latest tag only and remember to mention the tag i.e debian:latest, container should be named as volume-container-devops-2, and again run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/cluster.
4. Volume name should be volume-share of type emptyDir.
5. After creating the pod, exec into the first container i.e volume-container-devops-1, and just for testing create a file media.txt with any content under the mounted path of first container i.e /tmp/media.
6. The file media.txt should be present under the mounted path /tmp/cluster on the second container volume-container-devops-2 as well, since they are using a shared volume.

Solution:
Create the pods.yaml file and add the following content
apiVersion: v1
kind: Pod
metadata:
  name: volume-share-devops
spec:
  containers:
     - name: volume-container-devops-1
       image: debian:latest
       command: ["/bin/sh", "-c", "sleep 600;"]
       volumeMounts:
         - mountPath: /tmp/media
           name: volume-share
     - name: volume-container-devops-2
       image: debian:latest
       command: ["/bin/sh", "-c", "sleep 600;"]
       volumeMounts:
         - mountPath: /tmp/cluster
           name: volume-share
  volumes:
    - name: volume-share
      emptyDir:
        sizeLimit: 5Mi
Create the pod -> kubectl create -f pods.yaml
Connect to the shell of first container -> kubectl exec volume-share-devops -c volume-container-devops-1 -i -t -- bash
Create media.txt file in the location /tmp/media
Connect to the shell of the second container > kubectl exec volume-share-devops -c volume-container-devops-2 -i -t -- bash
Check if the file is present or not in the location /tmp/cluster
This happens because we are using a single volume share that is connected with two different location. This is the very generic use case of empty dir volumes

From the session
thor@jumphost ~$ kubectl exec volume-share-devops -c volume-container-devops-1 -i -t -- bash
root@volume-share-devops:/# cd /tmp/media/  
root@volume-share-devops:/tmp/media# touch media.txt
root@volume-share-devops:/tmp/media# 
root@volume-share-devops:/tmp/media# ls -lrth
total 0
-rw-r--r-- 1 root root 0 Feb  9 13:17 media.txt
root@volume-share-devops:/tmp/media# 
root@volume-share-devops:/tmp/media# exit
exit
thor@jumphost ~$ kubectl exec volume-share-devops -c volume-container-devops-2 -i -t -- bash
root@volume-share-devops:/# 
root@volume-share-devops:/# ls -lrth /tmp/cluster/
total 0
-rw-r--r-- 1 root root 0 Feb  9 13:17 media.txt
root@volume-share-devops:/# 
root@volume-share-devops:/# exit
exit
thor@jumphost ~$ 


Question 2: We have a web server container running the nginx image. The access and error logs generated by the web server are not critical enough to be placed on a persistent volume. However, Nautilus developers need access to the last 24 hours of logs so that they can trace issues and bugs. Therefore, we need to ship the access and error logs for the web server to a log-aggregation service. Following the separation of concerns principle, we implement the Sidecar pattern by deploying a second container that ships the error and access logs from nginx. Nginx does one thing, and it does it well—serving web pages. The second container also specializes in its task—shipping logs. Since containers are running on the same Pod, we can use a shared emptyDir volume to read and write logs.

1. Create a pod named webserver.
2. Create an emptyDir volume shared-logs.
3. Create two containers from nginx and ubuntu images with latest tag only and remember to mention tag i.e nginx:latest, nginx container name should be nginx-container and ubuntu container name should be sidecar-container on webserver pod.
4. Add command on sidecar-container "sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"
5. Mount the volume shared-logs on both containers at location /var/log/nginx, all containers should be up and running.

Solution:
Create pods.yaml file with the below content:

apiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  containers:
    - name: nginx-container
      image: nginx:latest
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
    - name: sidecar-container
      image: ubuntu:latest
      command: ["/bin/sh" , "-c" , "while true; do cat /var/log/nginx/access.log /var/log/ngin
x/error.log; sleep 30; done"]
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
  volumes:
    - name: shared-logs
      emptyDir: {}
run -> kubectl create -f pods.yaml
Once the pods are running. Enter into one sidecar container and check if the logs are present or not.
Log from side-car container.

thor@jumphost ~$ kubectl exec webserver -c sidecar-container -i -t -- bash
root@webserver:/# cd /var/log/nginx/
root@webserver:/var/log/nginx# ls -lrth
total 4.0K
-rw-r--r-- 1 root root    0 Feb  9 15:40 access.log
-rw-r--r-- 1 root root 1.3K Feb  9 15:40 error.log
root@webserver:/var/log/nginx# 

Question 3: Some of the Nautilus team developers are developing a static website and they want to deploy it on Kubernetes cluster. They want it to be highly available and scalable. Therefore, based on the requirements, the DevOps team has decided to create a deployment for it with multiple replicas. Below you can find more details about it:

1.Create a deployment using nginx image with latest tag only and remember to mention the tag i.e nginx:latest. Name it as nginx-deployment. The container should be named as nginx-container, also make sure replica counts are 3.
2.Create a NodePort type service named nginx-service. The nodePort should be 30011.

Solution.
Create the deployment file with both the content
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  selector:
    app: nginx  -> this maps the deployment with this service
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30011

run -> kubectl create -f deployment.yaml
check for the endpoints -> kubectl get endpoints nginx-service 
NAME            ENDPOINTS                                   AGE
nginx-service   10.244.0.5:80,10.244.0.6:80,10.244.0.7:80   8m14s   -- these are the ips of the pods of the deployment

Question 4: The Nautilus DevOps team is working on to setup some pre-requisites for an application that will send the greetings to different users. There is a sample deployment, that needs to be tested. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about it.
Create a pod named print-envars-greeting.

Configure spec as, the container name should be print-env-container and use bash image.

Create three environment variables:

a. GREETING and its value should be Welcome to

b. COMPANY and its value should be DevOps

c. GROUP and its value should be Datacenter

Use command ["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"'] (please use this exact command), also set its restartPolicy policy to Never to avoid crash loop back.

You can check the output using kubectl logs -f print-envars-greeting command.

Solution:
Create pods.yaml

apiVersion: v1
kind: Pod
metadata:
  name: print-envars-greeting
spec:
  containers:
    - name: print-env-container
      image: bash
      command: ["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']
      env:
        - name: GREETING
          value: "Welcome to"
        - name: COMPANY
          value: "DevOps"
        - name: GROUP
          value: "Datacenter"
  restartPolicy: Never

run -> kubectl create -f pods.yaml
check the logs for the output -> kubectl logs print-envars-greeting (below is the output when you run this)
Welcome to DevOps Datacenter

Question 5: There is a production deployment planned for next week. The Nautilus DevOps team wants to test the deployment update and rollback on Dev environment first so that they can identify the risks in advance. Below you can find more details about the plan they want to execute.

1.Create a namespace xfusion. Create a deployment called httpd-deploy under this new namespace, It should have one container called httpd, use httpd:2.4.28 image and 3 replicas. The deployment should use RollingUpdate strategy with maxSurge=1, and maxUnavailable=2. Also create a NodePort type service named httpd-service and expose the deployment on nodePort: 30008.
2.Now upgrade the deployment to version httpd:2.4.43 using a rolling update.
3.Finally, once all pods are updated undo the recent update and roll back to the previous/original version.

Solution:
Create deployment.yaml file with the below content

apiVersion: apps/v1
kind: Deployment
metadata:
 name: httpd-deploy
 namespace: xfusion
 labels:
   app: httpd
spec:
 replicas: 3
 selector:
   matchLabels:
     app: httpd
 template:
   metadata:
     labels:
       app: httpd
   spec:
     containers:
     - name: httpd
       image: httpd:2.4.28
       ports:
       - containerPort: 80
 strategy:
   type: RollingUpdate
   rollingUpdate:
     maxSurge: 1
     maxUnavailable: 2

---
apiVersion: v1
kind: Service
metadata:
  name: httpd-service
  namespace: xfusion
spec:
  type: NodePort
  selector:
    app: httpd
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30008

create the namespace -> kubectl create ns xfusion
run -> kubectl create -f deployment.yaml
check deployment and service endpoints -> kubectl get deployment --namespace xfusion and kubectl get endpoints httpd-service --namespace xfusion
you should see ips of all the pods of the deployment which is exposed by this service.
Update the image -> kubectl set image deployment/httpd-deploy httpd=httpd:2.4.43 --namespace xfusion
rollback the updates run -> kubectl rollout undo deployment/httpd-deploy --namespace xfusion
Done!

Question 6: The Nautilus DevOps team is planning to set up a Jenkins CI server to create/manage some deployment pipelines for some of the projects. They want to set up the Jenkins server on Kubernetes cluster. Below you can find more details about the task:

1) Create a namespace jenkins
2) Create a Service for jenkins deployment. Service name should be jenkins-service under jenkins namespace, type should be NodePort, nodePort should be 30008
3) Create a Jenkins Deployment under jenkins namespace, It should be name as jenkins-deployment , labels app should be jenkins , container name should be jenkins-container , use jenkins/jenkins image , containerPort should be 8080 and replicas count should be 1.

Solution:
Create the namespace -> kubectl create ns jenkins
Create the YAML file

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins-deployment
  namespace: jenkins
  labels:
    app: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
      - name: jenkins-container
        image: jenkins/jenkins
        ports:
          - containerPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: jenkins-service
  namespace: jenkins
spec:
  type: NodePort
  selector:
    app: jenkins
  ports:
    - port: 8080
      nodePort: 30008

kubectl create -f deployment.yaml
check the endpoints -> kubectl get endpoints jenkins-service --namespace jenkins 
NAME              ENDPOINTS         AGE
jenkins-service   10.244.0.5:8080   30s
Done


Question 7: The Nautilus DevOps teams is planning to set up a Grafana tool to collect and analyze analytics from some applications. They are planning to deploy it on Kubernetes cluster. Below you can find more details.

1.) Create a deployment named grafana-deployment-datacenter using any grafana image for Grafana app. Set other parameters as per your choice.
2.) Create NodePort type service with nodePort 32000 to expose the app.

Solution: 
Create the deployment file with the below content

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-deployment-datacente
  labels:
    app: grafana
spec:
  replicas: 2
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana-container
        image: grafana/grafana
        ports:
        - containerPort: 3000


---
apiVersion: v1
kind: Service
metadata:
  name: grafana-deployment-svc
spec:
  type: NodePort
  selector:
    app: grafana
  ports:
    - port: 3000
      nodePort: 32000

run -> kubectl create -f deployment.yaml
Check if the pods are registerd as endpoints with the svc -> kubectl get endpoints grafana-deployment-svc
Click on the icon and you can see the login page of Grafana.
Done!

Question 8: A new java-based application is ready to be deployed on a Kubernetes cluster. The development team had a meeting with the DevOps team to share the requirements and application scope. The team is ready to setup an application stack for it under their existing cluster. Below you can find the details for this:

1. Create a namespace named tomcat-namespace-nautilus.
2. Create a deployment for tomcat app which should be named as tomcat-deployment-nautilus under the same namespace you created. Replica count should be 1, the container should be named as tomcat-container-nautilus, its image should be gcr.io/kodekloud/centos-ssh-enabled:tomcat and its container port should be 8080.
3. Create a service for tomcat app which should be named as tomcat-service-nautilus under the same namespace you created. Service type should be NodePort and nodePort should be 32227.

Solution: 
Create the namespace -> kubectl create namespace tomcat-namespace-nautilus
Check namespace is active -> kubectl get namespace | grep tomcat-namespace-nautilus
Create the deployment file.
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment-nautilus
  namespace: tomcat-namespace-nautilus
  labels:
    app: tomcat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat
  template:
    metadata:
      labels:
        app: tomcat
    spec:
      containers:
      - name: tomcat-container-nautilus
        image: gcr.io/kodekloud/centos-ssh-enabled:tomcat
        ports:
        - containerPort: 8080


---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service-nautilus
  namespace: tomcat-namespace-nautilus
spec:
  type: NodePort
  selector:
    app: tomcat
  ports:
    - port: 8080
      nodePort: 32227

run -> kubectl create -f deployment.yaml
Check every resource is created -> kubectl get all --namespace tomcat-namespace-nautilus
Check the service endpoints -> kubectl get endpoints --namespace tomcat-namespace-nautilus
Everything is correct. You can access the application 
Done!

Question 9: The Nautilus development team has completed development of one of the node applications, which they are planning to deploy on a Kubernetes cluster. They recently had a meeting with the DevOps team to share their requirements. Based on that, the DevOps team has listed out the exact requirements to deploy the app. Find below more details:

1. Create a deployment using gcr.io/kodekloud/centos-ssh-enabled:node image, replica count must be 2.
2. Create a service to expose this app, the service type must be NodePort, targetPort must be 8080 and nodePort should be 30012.
3. Make sure all the pods are in Running state after the deployment.
4. You can check the application by clicking on NodeApp button on top bar.

Solution:
Create the deployment.yaml file
thor@jumphost ~$ cat deployments.yaml 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodeapp-deployment
  labels:
    app: nodeapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nodeapp
  template:
    metadata:
      labels:
        app: nodeapp
    spec:
      containers:
      - name: nodeapp-container
        image: gcr.io/kodekloud/centos-ssh-enabled:node
        ports:
        - containerPort: 8080


---
apiVersion: v1
kind: Service
metadata:
  name: nodeapp-service
spec:
  type: NodePort
  selector:
    app: nodeapp
  ports:
    - port: 8080
      nodePort: 30012

run -> kubectl create -f deployment.yaml
make sure all the resources are running -> kubectl get all
check the service endpoints -> kubectl get endpoints nodeapp-service
If everything is correct you can access the nodeapp and see the UI.
Done

Question 10: Last week, the Nautilus DevOps team deployed a redis app on Kubernetes cluster, which was working fine so far. This morning one of the team members was making some changes in this existing setup, but he made some mistakes and the app went down. We need to fix this as soon as possible. Please take a look.

The deployment name is redis-deployment. The pods are not in running state right now, so please look into the issue and fix the same.

Solution:
1. Checked the pod logs 
kubectl logs redis-deployment-54cdf4f76d-mm869
Error from server (BadRequest): container "redis-container" in pod "redis-deployment-54cdf4f76d-mm869" is waiting to start: ContainerCreating
2. Checked the deployment yaml file found out that the image name is not correct
spec:
      containers:
      - image: redis:alpin
3. Updated the image name from redis-alpin to redis-alpine
4. Also saw that the config map name was not correctly set update the configmap name under volumes in the deployment.
5. Checked the deployment and pod are in healthy state.
kubectl get all
NAME                                    READY   STATUS        RESTARTS   AGE
pod/redis-deployment-54f7cf4df7-9jv8m   0/1     Terminating   0          5m41s
pod/redis-deployment-7c8d4f6ddf-svb9w   1/1     Running       0          23s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   16m

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/redis-deployment   1/1     1            1           9m17s

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/redis-deployment-54cdf4f76d   0         0         0       9m17s
replicaset.apps/redis-deployment-54f7cf4df7   0         0         0       5m41s
replicaset.apps/redis-deployment-7c8d4f6ddf   1         1         1       23s

Troubleshooting Done!

Question 11: One of the DevOps team member was trying to install a WordPress website on a LAMP stack which is essentially deployed on Kubernetes cluster. It was working well and we could see the installation page a few hours ago. However something is messed up with the stack now due to a website went down. Please look into the issue and fix it:

FYI, deployment name is lamp-wp and its using a service named lamp-service. The Apache is using http default port and nodeport is 30008. From the application logs it has been identified that application is facing some issues while connecting to the database in addition to other issues. Additionally, there are some environment variables associated with the pods like MYSQL_ROOT_PASSWORD, MYSQL_DATABASE,  MYSQL_USER, MYSQL_PASSWORD, MYSQL_HOST.

Solution:
1. I checked all the resources status using kubectl get all
2. I checked the deployment yaml using kubectl get deployment lamp-wp -o yaml
3. I Checked the service deployment using kubectl get svc lamp-service -o yaml
First issue found out
Node port service is using 30009 port but it should use 30008 port as per the case
After changing that. Lamp service was able to connect to the MySQL service but still application was not able to connect to the database
Second issue found out
After checking the logs of the lamp application i found out the env variables are not set correctly. Below are the steps which i followed to resolve this
1. Checked the pod logs -> kubectl logs lamp-wp-56c7c454fc-9lrwl
Logs showed -> [10-Feb-2026 08:09:08] WARNING: [pool www] child 224 said into stderr: "NOTICE: PHP message: PHP Warning:  mysqli_connect(): (HY000/2005): Unknown MySQL server host 'mysql' (-2) in /app/index.php on line 8"
[Tue Feb 10 08:09:08.959260 2026] [proxy_fcgi:error] [pid 123:tid 140040222137008] [client 10.244.0.1:65260] AH01071: Got error 'PHP message: PHP Notice:  Undefined index: MYSQL_PASSWORDS in /app/index.php on line 4\nPHP message: PHP Notice:  Undefined index: HOST_MYQSL in /app/index.php on line 5\nPHP message: PHP Warning:  mysqli_connect(): (HY000/2005): Unknown MySQL server host 'mysql' (-2) in /app/index.php on line 8\n', referer: https://2gqxr4b5vx5v6cf7.labs.kodekloud.com/

If you see the values are not getting populated because of wrong config under /app/index.php
2. I connected to the shell session of the container using -> kubectl exec lamp-wp-56c7c454fc-9lrwl -c httpd-php-container -i -t -- bash
3. Checked the config -> cat /app/index.php
This is the config before change
bash-4.3# cat /app/index.php 
<?php
$dbname = $_ENV['MYSQL_DATABASE'];
$dbuser = $_ENV['MYSQL_USER'];
$dbpass = $_ENV['MYSQL_PASSWORDS'];
$dbhost = $_ENV['HOST_MYQSL'];


$connect = mysqli_connect($dbhost, $dbuser, $dbpass) or die("Unable to Connect to '$dbhost'");

$test_query = "SHOW TABLES FROM $dbname";
$result = mysqli_query($test_query);

if ($result->connect_error) {
   die("Connection failed: " . $conn->connect_error);
}

4. Edited the conf.
cat /app/index.php
<?php
$dbname = $_ENV['MYSQL_DATABASE'];
$dbuser = $_ENV['MYSQL_USER'];
$dbpass = $_ENV['MYSQL_PASSWORD'];
$dbhost = $_ENV['MYSQL_HOST'];


$connect = mysqli_connect($dbhost, $dbuser, $dbpass) or die("Unable to Connect to '$dbhost'");

$test_query = "SHOW TABLES FROM $dbname";
$result = mysqli_query($test_query);

if ($result->connect_error) {
   die("Connection failed: " . $conn->connect_error);
}
  echo "Connected successfully";bash-4.3# 
5. After saving the config LAMP application had connectivity. Nodeport service was able to access the clusterip service as they were in the same namespace.
Troubleshooting done.
