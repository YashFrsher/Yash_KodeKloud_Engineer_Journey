Question 1: We are working on an application that will be deployed on multiple containers within a pod on Kubernetes cluster. There is a requirement to share a volume among the containers to save some temporary data. The Nautilus DevOps team is developing a similar template to replicate the scenario. Below you can find more details about it.

1. Create a pod named volume-share-devops.
2. For the first container, use image debian with latest tag only and remember to mention the tag i.e debian:latest, container should be named as volume-container-devops-1, and run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/media.
3. For the second container, use image debian with the latest tag only and remember to mention the tag i.e debian:latest, container should be named as volume-container-devops-2, and again run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/cluster.
4. Volume name should be volume-share of type emptyDir.
5. After creating the pod, exec into the first container i.e volume-container-devops-1, and just for testing create a file media.txt with any content under the mounted path of first container i.e /tmp/media.
6. The file media.txt should be present under the mounted path /tmp/cluster on the second container volume-container-devops-2 as well, since they are using a shared volume.

Solution:
Create the pods.yaml file and add the following content
apiVersion: v1
kind: Pod
metadata:
  name: volume-share-devops
spec:
  containers:
     - name: volume-container-devops-1
       image: debian:latest
       command: ["/bin/sh", "-c", "sleep 600;"]
       volumeMounts:
         - mountPath: /tmp/media
           name: volume-share
     - name: volume-container-devops-2
       image: debian:latest
       command: ["/bin/sh", "-c", "sleep 600;"]
       volumeMounts:
         - mountPath: /tmp/cluster
           name: volume-share
  volumes:
    - name: volume-share
      emptyDir:
        sizeLimit: 5Mi
Create the pod -> kubectl create -f pods.yaml
Connect to the shell of first container -> kubectl exec volume-share-devops -c volume-container-devops-1 -i -t -- bash
Create media.txt file in the location /tmp/media
Connect to the shell of the second container > kubectl exec volume-share-devops -c volume-container-devops-2 -i -t -- bash
Check if the file is present or not in the location /tmp/cluster
This happens because we are using a single volume share that is connected with two different location. This is the very generic use case of empty dir volumes

From the session
thor@jumphost ~$ kubectl exec volume-share-devops -c volume-container-devops-1 -i -t -- bash
root@volume-share-devops:/# cd /tmp/media/  
root@volume-share-devops:/tmp/media# touch media.txt
root@volume-share-devops:/tmp/media# 
root@volume-share-devops:/tmp/media# ls -lrth
total 0
-rw-r--r-- 1 root root 0 Feb  9 13:17 media.txt
root@volume-share-devops:/tmp/media# 
root@volume-share-devops:/tmp/media# exit
exit
thor@jumphost ~$ kubectl exec volume-share-devops -c volume-container-devops-2 -i -t -- bash
root@volume-share-devops:/# 
root@volume-share-devops:/# ls -lrth /tmp/cluster/
total 0
-rw-r--r-- 1 root root 0 Feb  9 13:17 media.txt
root@volume-share-devops:/# 
root@volume-share-devops:/# exit
exit
thor@jumphost ~$ 


Question 2: We have a web server container running the nginx image. The access and error logs generated by the web server are not critical enough to be placed on a persistent volume. However, Nautilus developers need access to the last 24 hours of logs so that they can trace issues and bugs. Therefore, we need to ship the access and error logs for the web server to a log-aggregation service. Following the separation of concerns principle, we implement the Sidecar pattern by deploying a second container that ships the error and access logs from nginx. Nginx does one thing, and it does it well—serving web pages. The second container also specializes in its task—shipping logs. Since containers are running on the same Pod, we can use a shared emptyDir volume to read and write logs.

1. Create a pod named webserver.
2. Create an emptyDir volume shared-logs.
3. Create two containers from nginx and ubuntu images with latest tag only and remember to mention tag i.e nginx:latest, nginx container name should be nginx-container and ubuntu container name should be sidecar-container on webserver pod.
4. Add command on sidecar-container "sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"
5. Mount the volume shared-logs on both containers at location /var/log/nginx, all containers should be up and running.

Solution:
Create pods.yaml file with the below content:

apiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  containers:
    - name: nginx-container
      image: nginx:latest
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
    - name: sidecar-container
      image: ubuntu:latest
      command: ["/bin/sh" , "-c" , "while true; do cat /var/log/nginx/access.log /var/log/ngin
x/error.log; sleep 30; done"]
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
  volumes:
    - name: shared-logs
      emptyDir: {}
run -> kubectl create -f pods.yaml
Once the pods are running. Enter into one sidecar container and check if the logs are present or not.
Log from side-car container.

thor@jumphost ~$ kubectl exec webserver -c sidecar-container -i -t -- bash
root@webserver:/# cd /var/log/nginx/
root@webserver:/var/log/nginx# ls -lrth
total 4.0K
-rw-r--r-- 1 root root    0 Feb  9 15:40 access.log
-rw-r--r-- 1 root root 1.3K Feb  9 15:40 error.log
root@webserver:/var/log/nginx# 

Question 3: Some of the Nautilus team developers are developing a static website and they want to deploy it on Kubernetes cluster. They want it to be highly available and scalable. Therefore, based on the requirements, the DevOps team has decided to create a deployment for it with multiple replicas. Below you can find more details about it:

1.Create a deployment using nginx image with latest tag only and remember to mention the tag i.e nginx:latest. Name it as nginx-deployment. The container should be named as nginx-container, also make sure replica counts are 3.
2.Create a NodePort type service named nginx-service. The nodePort should be 30011.

Solution.
Create the deployment file with both the content
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  selector:
    app: nginx  -> this maps the deployment with this service
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30011

run -> kubectl create -f deployment.yaml
check for the endpoints -> kubectl get endpoints nginx-service 
NAME            ENDPOINTS                                   AGE
nginx-service   10.244.0.5:80,10.244.0.6:80,10.244.0.7:80   8m14s   -- these are the ips of the pods of the deployment

Question 4: The Nautilus DevOps team is working on to setup some pre-requisites for an application that will send the greetings to different users. There is a sample deployment, that needs to be tested. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about it.
Create a pod named print-envars-greeting.

Configure spec as, the container name should be print-env-container and use bash image.

Create three environment variables:

a. GREETING and its value should be Welcome to

b. COMPANY and its value should be DevOps

c. GROUP and its value should be Datacenter

Use command ["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"'] (please use this exact command), also set its restartPolicy policy to Never to avoid crash loop back.

You can check the output using kubectl logs -f print-envars-greeting command.

Solution:
Create pods.yaml

apiVersion: v1
kind: Pod
metadata:
  name: print-envars-greeting
spec:
  containers:
    - name: print-env-container
      image: bash
      command: ["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']
      env:
        - name: GREETING
          value: "Welcome to"
        - name: COMPANY
          value: "DevOps"
        - name: GROUP
          value: "Datacenter"
  restartPolicy: Never

run -> kubectl create -f pods.yaml
check the logs for the output -> kubectl logs print-envars-greeting (below is the output when you run this)
Welcome to DevOps Datacenter

Question 5: There is a production deployment planned for next week. The Nautilus DevOps team wants to test the deployment update and rollback on Dev environment first so that they can identify the risks in advance. Below you can find more details about the plan they want to execute.

1.Create a namespace xfusion. Create a deployment called httpd-deploy under this new namespace, It should have one container called httpd, use httpd:2.4.28 image and 3 replicas. The deployment should use RollingUpdate strategy with maxSurge=1, and maxUnavailable=2. Also create a NodePort type service named httpd-service and expose the deployment on nodePort: 30008.
2.Now upgrade the deployment to version httpd:2.4.43 using a rolling update.
3.Finally, once all pods are updated undo the recent update and roll back to the previous/original version.

Solution:
Create deployment.yaml file with the below content

apiVersion: apps/v1
kind: Deployment
metadata:
 name: httpd-deploy
 namespace: xfusion
 labels:
   app: httpd
spec:
 replicas: 3
 selector:
   matchLabels:
     app: httpd
 template:
   metadata:
     labels:
       app: httpd
   spec:
     containers:
     - name: httpd
       image: httpd:2.4.28
       ports:
       - containerPort: 80
 strategy:
   type: RollingUpdate
   rollingUpdate:
     maxSurge: 1
     maxUnavailable: 2

---
apiVersion: v1
kind: Service
metadata:
  name: httpd-service
  namespace: xfusion
spec:
  type: NodePort
  selector:
    app: httpd
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30008

create the namespace -> kubectl create ns xfusion
run -> kubectl create -f deployment.yaml
check deployment and service endpoints -> kubectl get deployment --namespace xfusion and kubectl get endpoints httpd-service --namespace xfusion
you should see ips of all the pods of the deployment which is exposed by this service.
Update the image -> kubectl set image deployment/httpd-deploy httpd=httpd:2.4.43 --namespace xfusion
rollback the updates run -> kubectl rollout undo deployment/httpd-deploy --namespace xfusion
Done!

Question 6: The Nautilus DevOps team is planning to set up a Jenkins CI server to create/manage some deployment pipelines for some of the projects. They want to set up the Jenkins server on Kubernetes cluster. Below you can find more details about the task:

1) Create a namespace jenkins
2) Create a Service for jenkins deployment. Service name should be jenkins-service under jenkins namespace, type should be NodePort, nodePort should be 30008
3) Create a Jenkins Deployment under jenkins namespace, It should be name as jenkins-deployment , labels app should be jenkins , container name should be jenkins-container , use jenkins/jenkins image , containerPort should be 8080 and replicas count should be 1.

Solution:
Create the namespace -> kubectl create ns jenkins
Create the YAML file

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins-deployment
  namespace: jenkins
  labels:
    app: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
      - name: jenkins-container
        image: jenkins/jenkins
        ports:
          - containerPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: jenkins-service
  namespace: jenkins
spec:
  type: NodePort
  selector:
    app: jenkins
  ports:
    - port: 8080
      nodePort: 30008

kubectl create -f deployment.yaml
check the endpoints -> kubectl get endpoints jenkins-service --namespace jenkins 
NAME              ENDPOINTS         AGE
jenkins-service   10.244.0.5:8080   30s
Done
